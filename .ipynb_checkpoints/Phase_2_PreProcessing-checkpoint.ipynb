{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Phase 2 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the previous phase, we realized that the probability of reorder for Produce, Dairy Eggs, Beverages and Snacks is the highest. We'll see if our models can prove this. Moreover, since we used Google Colab, the work had to split across multiple notebooks for the different tasks. However, since we have not attached a seperate document, we are explaining everything that we are doing alongside the code in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split;\n",
    "%matplotlib inline\n",
    "\n",
    "data_dir = 'data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing for Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32434489, 4)\n",
      "   order_id  user_id eval_set  order_number  order_dow  order_hour_of_day  \\\n",
      "0   2539329        1    prior             1          2                  8   \n",
      "1   2398795        1    prior             2          3                  7   \n",
      "2    473747        1    prior             3          3                 12   \n",
      "3   2254736        1    prior             4          4                  7   \n",
      "4    431534        1    prior             5          4                 15   \n",
      "\n",
      "   days_since_prior_order  \n",
      "0                     NaN  \n",
      "1                    15.0  \n",
      "2                    21.0  \n",
      "3                    29.0  \n",
      "4                    28.0  \n"
     ]
    }
   ],
   "source": [
    "aisles = pd.read_csv(data_dir+'aisles.csv')\n",
    "departments = pd.read_csv(data_dir+'departments.csv')\n",
    "\n",
    "products = pd.read_csv(data_dir+'products.csv')\n",
    "orders = pd.read_csv(data_dir+'orders.csv')\n",
    "\n",
    "order_products_prior = pd.read_csv(data_dir+'order_products__prior.csv')\n",
    "print(order_products_prior.shape)\n",
    "print(orders.head())\n",
    "# We have already seen what they look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(964462, 7)\n",
      "(39362, 7)\n",
      "(22500, 7)\n"
     ]
    }
   ],
   "source": [
    "orders_prior = orders[orders['eval_set'] == 'prior']\n",
    "# Reducing size of dataset so it can run on our laptops/Colab\n",
    "orders_prior, _ = train_test_split(orders_prior, test_size=0.7)\n",
    "print(orders_prior.shape)\n",
    "orders_train = orders[orders['eval_set'] == 'train']\n",
    "orders_train, _ = train_test_split(orders_train, test_size=0.7)\n",
    "print(orders_train.shape)\n",
    "orders_test = orders[orders['eval_set'] == 'test']\n",
    "orders_test, _ = train_test_split(orders_test, test_size=0.7)\n",
    "print(orders_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1026324, 7)\n"
     ]
    }
   ],
   "source": [
    "orders, _ = train_test_split(orders, test_size=0.7)\n",
    "print(orders.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going to merge everything into one big dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9737270, 15)\n"
     ]
    }
   ],
   "source": [
    "all_data = pd.merge(products, aisles, on='aisle_id')\n",
    "all_data = pd.merge(all_data, departments, on='department_id')\n",
    "\n",
    "all_orders = pd.merge(order_products_prior, orders, on='order_id')\n",
    "all_data = pd.merge(all_data, all_orders, on='product_id')\n",
    "print(all_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   product_id                product_name  aisle_id  department_id  \\\n",
      "0           1  Chocolate Sandwich Cookies        61             19   \n",
      "1           1  Chocolate Sandwich Cookies        61             19   \n",
      "2           1  Chocolate Sandwich Cookies        61             19   \n",
      "3           1  Chocolate Sandwich Cookies        61             19   \n",
      "4           1  Chocolate Sandwich Cookies        61             19   \n",
      "\n",
      "           aisle department  order_id  add_to_cart_order  reordered  user_id  \\\n",
      "0  cookies cakes     snacks      9273                 30          0    50005   \n",
      "1  cookies cakes     snacks     11140                  1          1    63782   \n",
      "2  cookies cakes     snacks     14668                 13          1   106519   \n",
      "3  cookies cakes     snacks     19479                  5          0   110984   \n",
      "4  cookies cakes     snacks     19879                 12          0   142388   \n",
      "\n",
      "  eval_set  order_number  order_dow  order_hour_of_day  days_since_prior_order  \n",
      "0    prior             1          1                 15                     NaN  \n",
      "1    prior             4          1                 14                    14.0  \n",
      "2    prior            18          0                 22                     7.0  \n",
      "3    prior             2          2                 12                    20.0  \n",
      "4    prior             7          3                  5                    15.0  \n"
     ]
    }
   ],
   "source": [
    "print(all_data.head())\n",
    "all_data.to_pickle('all_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['product_id', 'aisle_id', 'department_id', 'aisle', 'department', 'order_id', 'add_to_cart_order', 'user_id', 'eval_set', 'order_number', 'order_dow', 'order_hour_of_day', 'days_since_prior_order']\n",
      "   product_id  aisle_id  department_id          aisle department  order_id  \\\n",
      "0           1        61             19  cookies cakes     snacks      9273   \n",
      "1           1        61             19  cookies cakes     snacks     11140   \n",
      "2           1        61             19  cookies cakes     snacks     14668   \n",
      "3           1        61             19  cookies cakes     snacks     19479   \n",
      "4           1        61             19  cookies cakes     snacks     19879   \n",
      "\n",
      "   add_to_cart_order  user_id eval_set  order_number  order_dow  \\\n",
      "0                 30    50005    prior             1          1   \n",
      "1                  1    63782    prior             4          1   \n",
      "2                 13   106519    prior            18          0   \n",
      "3                  5   110984    prior             2          2   \n",
      "4                 12   142388    prior             7          3   \n",
      "\n",
      "   order_hour_of_day  days_since_prior_order  \n",
      "0                 15                     NaN  \n",
      "1                 14                    14.0  \n",
      "2                 22                     7.0  \n",
      "3                 12                    20.0  \n",
      "4                  5                    15.0  \n",
      "\n",
      "0    0\n",
      "1    1\n",
      "2    1\n",
      "3    0\n",
      "4    0\n",
      "Name: reordered, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "all_columns = list(all_data.columns)\n",
    "features = [column for column in all_columns \n",
    "            if column != 'reordered' if column != 'product_name']\n",
    "print(features)\n",
    "\n",
    "X = all_data[features]\n",
    "Y = all_data['reordered']\n",
    "\n",
    "print(X.head())\n",
    "print()\n",
    "print(Y.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['product_id', 'aisle_id', 'department_id', 'order_id', 'add_to_cart_order', 'user_id', 'order_number', 'order_dow', 'order_hour_of_day', 'days_since_prior_order']\n",
      "(9737270, 10)\n",
      "product_id                  int64\n",
      "aisle_id                    int64\n",
      "department_id               int64\n",
      "order_id                    int64\n",
      "add_to_cart_order           int64\n",
      "user_id                     int64\n",
      "order_number                int64\n",
      "order_dow                   int64\n",
      "order_hour_of_day           int64\n",
      "days_since_prior_order    float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# I will be getting rid of string variables since they are not important and each\n",
    "# already has an ID associated with it\n",
    "relevant_features = [feature for feature in features \n",
    "                        if feature != 'eval_set'\n",
    "                        if feature != 'aisle'\n",
    "                        if feature != 'department']\n",
    "\n",
    "# This is done for the days_since_prior_order column\n",
    "X = X.fillna(0)\n",
    "X = X[relevant_features]\n",
    "print(relevant_features)\n",
    "print(X.shape)\n",
    "print(X.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting rid of all string variables is important over here since they do not provide us with any extra information regarding the data. We could have easily one-hot-encoded them to use them as categorical variables, however, product_name is useless when product_id is available. Thus we chose to simply remove these variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features in order of importance: \n",
      "Score of  order_number  is  1010198.3543936191\n",
      "Score of  add_to_cart_order  is  176875.69895921985\n",
      "Score of  department_id  is  15341.135016103908\n",
      "Score of  days_since_prior_order  is  5973.363510578231\n",
      "Score of  order_hour_of_day  is  4789.690540638999\n",
      "Score of  order_dow  is  486.3513015426374\n",
      "Score of  aisle_id  is  144.44622792698704\n",
      "Score of  product_id  is  131.87103173926747\n",
      "Score of  user_id  is  16.03905835234687\n",
      "Score of  order_id  is  3.8480886129524037\n",
      "['product_id' 'aisle_id' 'department_id' 'order_id' 'add_to_cart_order'\n",
      " 'user_id' 'order_number' 'order_dow' 'order_hour_of_day'\n",
      " 'days_since_prior_order']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "skb = SelectKBest(f_classif,k='all').fit(X,Y)\n",
    "scores = skb.scores_\n",
    "all_features = X.columns.values\n",
    "sort_index = np.argsort(scores)[::-1]\n",
    "ranked_features = []\n",
    "print (\"Features in order of importance: \")\n",
    "for x in sort_index:\n",
    "    print (\"Score of \",all_features[x],\" is \",scores[x])\n",
    "    ranked_features.append(all_features[x])\n",
    "print (all_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can say here, the score for order_id and user_id is extremely less, especially as compared to the other variables, so we will be removing those from our features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['order_number', 'add_to_cart_order', 'department_id', 'days_since_prior_order', 'order_hour_of_day', 'order_dow', 'aisle_id', 'product_id']\n",
      "(9737270, 8)\n"
     ]
    }
   ],
   "source": [
    "best_features = ranked_features[0:8]\n",
    "print(best_features)\n",
    "X_best = X[best_features]\n",
    "print(X_best.shape)\n",
    "\n",
    "X_best.to_pickle('X.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9737270,)\n"
     ]
    }
   ],
   "source": [
    "Y.to_pickle('Y.pkl')\n",
    "\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are converting the final dataframes into a pickle file as we go along. This is so that when we run our models on Google Colab we can upload just the pickle file to our Google Drive in order to test our models since our laptops do not have enough computing power to run the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing to note is that we have not used PCA in this case. This is because while PCA might make our model a little more accurate, it will also turn it into a \"black-box\", in the sense that if we wanted to gauge what variables were driving the result - i.e. what things are important if we want a certain product to be reordered, we could not have done that since PCA gives us completely new dimensions for the dataset. Moreover, PCA is also not very useful here since the number of features is already very less."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
